{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12090933,"sourceType":"datasetVersion","datasetId":7611421},{"sourceId":12095825,"sourceType":"datasetVersion","datasetId":7614660},{"sourceId":12095860,"sourceType":"datasetVersion","datasetId":7614687},{"sourceId":12095974,"sourceType":"datasetVersion","datasetId":7614766},{"sourceId":12096951,"sourceType":"datasetVersion","datasetId":7615456}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:16:58.140129Z","iopub.execute_input":"2025-06-08T08:16:58.140294Z","iopub.status.idle":"2025-06-08T08:17:01.032226Z","shell.execute_reply.started":"2025-06-08T08:16:58.140279Z","shell.execute_reply":"2025-06-08T08:17:01.031610Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/tinyllama-empathetic-sft-v41/adapter_model.safetensors\n/kaggle/input/tinyllama-empathetic-sft-v41/training_args.bin\n/kaggle/input/tinyllama-empathetic-sft-v41/adapter_config.json\n/kaggle/input/tinyllama-empathetic-sft-v41/README.md\n/kaggle/input/tinyllama-empathetic-sft-v41/tokenizer.json\n/kaggle/input/tinyllama-empathetic-sft-v41/tokenizer_config.json\n/kaggle/input/tinyllama-empathetic-sft-v41/special_tokens_map.json\n/kaggle/input/tinyllama-empathetic-sft-v41/tokenizer.model\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/adapter_model.safetensors\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/trainer_state.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/training_args.bin\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/adapter_config.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/README.md\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/tokenizer.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/tokenizer_config.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/scheduler.pt\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/special_tokens_map.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/optimizer.pt\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/rng_state.pth\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-186/tokenizer.model\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/adapter_model.safetensors\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/trainer_state.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/training_args.bin\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/adapter_config.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/README.md\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/tokenizer.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/tokenizer_config.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/scheduler.pt\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/special_tokens_map.json\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/optimizer.pt\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/rng_state.pth\n/kaggle/input/tinyllama-empathetic-sft-v41/checkpoint-126/tokenizer.model\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Download and extract the dataset\n!wget https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\n!mkdir -p empatheticdialogues\n!tar -xzf empatheticdialogues.tar.gz -C empatheticdialogues","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:17:01.033909Z","iopub.execute_input":"2025-06-08T08:17:01.034487Z","iopub.status.idle":"2025-06-08T08:17:02.937998Z","shell.execute_reply.started":"2025-06-08T08:17:01.034465Z","shell.execute_reply":"2025-06-08T08:17:02.937036Z"}},"outputs":[{"name":"stdout","text":"--2025-06-08 08:17:01--  https://dl.fbaipublicfiles.com/parlai/empatheticdialogues/empatheticdialogues.tar.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.238.176.44, 18.238.176.19, 18.238.176.115, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.238.176.44|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 28022709 (27M) [application/gzip]\nSaving to: ‘empatheticdialogues.tar.gz’\n\nempatheticdialogues 100%[===================>]  26.72M  55.7MB/s    in 0.5s    \n\n2025-06-08 08:17:01 (55.7 MB/s) - ‘empatheticdialogues.tar.gz’ saved [28022709/28022709]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load all CSVs\ntrain_df = pd.read_csv(\"empatheticdialogues/empatheticdialogues/train.csv\", quotechar='\"', escapechar='\\\\', on_bad_lines='skip')\nvalid_df = pd.read_csv(\"empatheticdialogues/empatheticdialogues/valid.csv\", quotechar='\"', escapechar='\\\\', on_bad_lines='skip')\ntest_df  = pd.read_csv(\"empatheticdialogues/empatheticdialogues/test.csv\", quotechar='\"', escapechar='\\\\', on_bad_lines='skip')\n\n# Create \"Instruction: <context>\\nResponse: <utterance>\" format\ntrain_df[\"text\"] = train_df.apply(lambda row: f\"Instruction: {row['context']}\\nResponse: {row['utterance']}\", axis=1)\nvalid_df[\"text\"] = valid_df.apply(lambda row: f\"Instruction: {row['context']}\\nResponse: {row['utterance']}\", axis=1)\n\n# Subset for Colab training\ntrain_sample = train_df.sample(n=5000, random_state=42)\nvalid_sample = valid_df.sample(n=500, random_state=42)\n\n# Convert to Hugging Face Datasets\ntrain_dataset = Dataset.from_pandas(train_sample[[\"text\"]])\nval_dataset = Dataset.from_pandas(valid_sample[[\"text\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:17:02.939246Z","iopub.execute_input":"2025-06-08T08:17:02.939591Z","iopub.status.idle":"2025-06-08T08:17:06.902119Z","shell.execute_reply.started":"2025-06-08T08:17:02.939553Z","shell.execute_reply":"2025-06-08T08:17:06.901560Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/990961786.py:5: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n  train_df = pd.read_csv(\"empatheticdialogues/empatheticdialogues/train.csv\", quotechar='\"', escapechar='\\\\', on_bad_lines='skip')\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q transformers datasets peft accelerate evaluate bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:17:06.902923Z","iopub.execute_input":"2025-06-08T08:17:06.903170Z","iopub.status.idle":"2025-06-08T08:18:44.390202Z","shell.execute_reply.started":"2025-06-08T08:17:06.903140Z","shell.execute_reply":"2025-06-08T08:18:44.389414Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport psutil\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback\nfrom peft import LoraConfig, TaskType, get_peft_model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef print_memory():\n    if torch.cuda.is_available():\n        print(f\"[GPU] Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB | Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n    mem = psutil.virtual_memory()\n    print(f\"[CPU] Used: {mem.used / 1e9:.2f} GB / {mem.total / 1e9:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:18:44.391336Z","iopub.execute_input":"2025-06-08T08:18:44.392046Z","iopub.status.idle":"2025-06-08T08:19:24.093597Z","shell.execute_reply.started":"2025-06-08T08:18:44.392014Z","shell.execute_reply":"2025-06-08T08:19:24.092936Z"}},"outputs":[{"name":"stderr","text":"2025-06-08 08:19:03.621649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749370744.082765      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749370744.209850      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:19:24.094313Z","iopub.execute_input":"2025-06-08T08:19:24.094933Z","iopub.status.idle":"2025-06-08T08:19:24.098485Z","shell.execute_reply.started":"2025-06-08T08:19:24.094911Z","shell.execute_reply":"2025-06-08T08:19:24.097755Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\ntokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:19:24.100790Z","iopub.execute_input":"2025-06-08T08:19:24.100972Z","iopub.status.idle":"2025-06-08T08:19:26.939707Z","shell.execute_reply.started":"2025-06-08T08:19:24.100958Z","shell.execute_reply":"2025-06-08T08:19:26.938906Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a1ee562355a48a3aef444e17bb09263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b57654deb9114639a23ebad73bd21182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5001d0cba021483db943311ffdd6fcc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"133eca663aba442692b8549c7a7a0de5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8acf8d234f9642e9b927cf549014954a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d6513514bf24ae1a66b1376cef5768a"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#best_conf\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16).to(device)\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32, \n    lora_dropout=0.05, \n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=TaskType.CAUSAL_LM,\n)\nmodel = get_peft_model(model, lora_config)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:19:26.940622Z","iopub.execute_input":"2025-06-08T08:19:26.941170Z","iopub.status.idle":"2025-06-08T08:19:38.970694Z","shell.execute_reply.started":"2025-06-08T08:19:26.941148Z","shell.execute_reply":"2025-06-08T08:19:38.969808Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f88a65f3ae46d9ab5c4e3eaa4f51dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76afd0b088e042c790bc9f14a4d00150"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b66410b57c64844bae720547672d389"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./tinyllama-empathetic-sft-v8\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    learning_rate=4e-5, \n    warmup_ratio=0.05,\n    weight_decay=0.01,\n    gradient_accumulation_steps=2,  \n    bf16=True,  \n    lr_scheduler_type=\"cosine\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=50,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    report_to=\"none\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:19:38.971555Z","iopub.execute_input":"2025-06-08T08:19:38.971763Z","iopub.status.idle":"2025-06-08T08:19:39.007820Z","shell.execute_reply.started":"2025-06-08T08:19:38.971747Z","shell.execute_reply":"2025-06-08T08:19:39.007081Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import time\n\nstart_time = time.time()  # Start the timer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\n\nprint(\"Starting supervised fine-tuning...\")\ntrainer.train()\n\nend_time = time.time()  # End the timer\nelapsed_time = end_time - start_time\n\n# Convert to hours, minutes, and seconds for readability\nhours, rem = divmod(elapsed_time, 3600)\nminutes, seconds = divmod(rem, 60)\nprint(f\"Training completed in {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d} (hh:mm:ss)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T08:19:39.008622Z","iopub.execute_input":"2025-06-08T08:19:39.008866Z","iopub.status.idle":"2025-06-08T10:46:04.996261Z","shell.execute_reply.started":"2025-06-08T08:19:39.008847Z","shell.execute_reply":"2025-06-08T10:46:04.995516Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/765275579.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting supervised fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [936/936 2:26:15, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.972900</td>\n      <td>2.025036</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.906200</td>\n      <td>1.997857</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training completed in 02:26:25 (hh:mm:ss)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\ndef print_memory():\n    if torch.cuda.is_available():\n        print(f\"[GPU] Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB | Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n    mem = psutil.virtual_memory()\n    print(f\"[CPU] Used: {mem.used / 1e9:.2f} GB / {mem.total / 1e9:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T02:13:06.202640Z","iopub.execute_input":"2025-06-07T02:13:06.202915Z","iopub.status.idle":"2025-06-07T02:13:06.208329Z","shell.execute_reply.started":"2025-06-07T02:13:06.202892Z","shell.execute_reply":"2025-06-07T02:13:06.207585Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"I had a rough day at work and I’m feeling exhausted.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T22:33:47.825630Z","iopub.execute_input":"2025-06-07T22:33:47.825949Z","iopub.status.idle":"2025-06-07T22:33:51.921025Z","shell.execute_reply.started":"2025-06-07T22:33:47.825926Z","shell.execute_reply":"2025-06-07T22:33:51.920107Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nI had a rough day at work and I’m feeling exhausted.\n<|assistant|>\nI understand how you feel. It's okay to take a break and rest. You can take a nap or go for a walk to recharge. You can also try to relax and do something that you enjoy. Remember to take care of yourself and give yourself some time to rest.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"I feel like no one really understands what I’m going through.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: I'm sorry you're feeling that way. It might help to talk to someone you trust or write your thoughts down. You're not alone, and expressing yourself is a good step.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:47:21.467041Z","iopub.execute_input":"2025-06-08T10:47:21.467324Z","iopub.status.idle":"2025-06-08T10:47:28.694748Z","shell.execute_reply.started":"2025-06-08T10:47:21.467303Z","shell.execute_reply":"2025-06-08T10:47:28.693813Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nI feel like no one really understands what I’m going through.\n<|assistant|>\nI understand that feeling. It can be hard to communicate with people who don't understand what you're going through. It's good to have someone who can help you understand.  I'm glad you're doing well.  I hope you're able to find some peace and solace.  Good luck!  I hope you find the support you need.  I'm sure you'll get through this.  You're a strong person.  You'll get through this. \n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"My best friend hasn’t talked to me in days, and it’s making me anxious.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: It's understandable to feel anxious. Try reaching out with a simple message — sometimes people get busy, but it doesn't mean they don't care. Open communication helps.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T22:33:02.620127Z","iopub.execute_input":"2025-06-07T22:33:02.620427Z","iopub.status.idle":"2025-06-07T22:33:09.511003Z","shell.execute_reply.started":"2025-06-07T22:33:02.620410Z","shell.execute_reply":"2025-06-07T22:33:09.510269Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nMy best friend hasn’t talked to me in days, and it’s making me anxious.\n<|assistant|>\nIt sounds like you are feeling anxious about your friend's behavior. It's understandable to feel that way. It's common for people to feel anxious when they don't hear from someone they care about. It's important to remember that it's okay to feel that way and that you're not alone. You can try to reach out to your friend and see if they're okay. If they're not, it's okay to ask for their perspective and try\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"I’m overwhelmed with school and feel like I’m falling behind.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: Take a deep breath — you're doing your best. Try breaking your tasks into smaller pieces and focus on one thing at a time. Progress happens step by step.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T22:33:24.316069Z","iopub.execute_input":"2025-06-07T22:33:24.316789Z","iopub.status.idle":"2025-06-07T22:33:30.815341Z","shell.execute_reply.started":"2025-06-07T22:33:24.316756Z","shell.execute_reply":"2025-06-07T22:33:30.814398Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nI’m overwhelmed with school and feel like I’m falling behind.\n<|assistant|>\nI understand how you feel. It can be really tough to balance school and other responsibilities. However, it's important to prioritize your time and make sure you're doing the things that are most important to you. It's also helpful to take breaks and prioritize self-care. Remember that you're doing a great job and that you'll get there eventually. Don't give up!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"I tried really hard on my project, but it didn’t turn out the way I hoped.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: That can be frustrating. What's important is that you put in the effort — that's how we grow. Try to reflect on what went well and what you could improve next time.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T02:13:21.904590Z","iopub.execute_input":"2025-06-07T02:13:21.904800Z","iopub.status.idle":"2025-06-07T02:13:25.495158Z","shell.execute_reply.started":"2025-06-07T02:13:21.904783Z","shell.execute_reply":"2025-06-07T02:13:25.494586Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nI tried really hard on my project, but it didn’t turn out the way I hoped.\n<|assistant|>\nI'm sorry to hear that. It sounds like you put a lot of effort into it. It's always frustrating when you don't get the results you want. I hope you learned something from the experience and that you can come back to it with a fresh perspective. Good luck!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"I got some bad news about a family member today.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: I'm really sorry to hear that. Make sure to take care of yourself and reach out for support if you need it. Talking to a loved one can be very comforting.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T02:13:25.495830Z","iopub.execute_input":"2025-06-07T02:13:25.496046Z","iopub.status.idle":"2025-06-07T02:13:28.106757Z","shell.execute_reply.started":"2025-06-07T02:13:25.496022Z","shell.execute_reply":"2025-06-07T02:13:28.106174Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nI got some bad news about a family member today.\n<|assistant|>\nThat's terrible news. I'm sorry to hear that. What happened? Did they pass away? Did they have any children? Did they have any other family members? I hope you're doing okay.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"I feel really lonely these days.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: Loneliness can be hard. Try engaging in activities you enjoy or joining a group with shared interests. Connecting with others takes time, but it’s possible.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T22:34:08.510159Z","iopub.execute_input":"2025-06-07T22:34:08.510978Z","iopub.status.idle":"2025-06-07T22:34:11.514478Z","shell.execute_reply.started":"2025-06-07T22:34:08.510943Z","shell.execute_reply":"2025-06-07T22:34:11.513706Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nI feel really lonely these days.\n<|assistant|>\nI understand how you feel. It's hard to be alone sometimes. But you can always reach out to friends or family for support. It's okay to ask for help when you need it.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"Everyone expects me to be strong, but I’m really struggling inside.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: It’s okay to feel that way. Even strong people need support. Try opening up to someone you trust — you don’t have to carry everything alone.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T02:13:30.910237Z","iopub.execute_input":"2025-06-07T02:13:30.910484Z","iopub.status.idle":"2025-06-07T02:13:34.840989Z","shell.execute_reply.started":"2025-06-07T02:13:30.910466Z","shell.execute_reply":"2025-06-07T02:13:34.840293Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nEveryone expects me to be strong, but I’m really struggling inside.\n<|assistant|>\nIt sounds like you are dealing with some personal issues. It is good to know that you are seeking help. It is always better to seek help when you are struggling. You can talk to someone about it and get the support you need. It is not easy to be strong, but it is important to be strong for yourself and others.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"My partner and I had a big argument and now I feel emotionally drained.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: Arguments can be really exhausting. Take some time to cool down and reflect. When you’re ready, try to have a calm, honest conversation about how you feel.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T02:13:34.841780Z","iopub.execute_input":"2025-06-07T02:13:34.842050Z","iopub.status.idle":"2025-06-07T02:13:38.442073Z","shell.execute_reply.started":"2025-06-07T02:13:34.842026Z","shell.execute_reply":"2025-06-07T02:13:38.441437Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nMy partner and I had a big argument and now I feel emotionally drained.\n<|assistant|>\nI'm sorry to hear that. It sounds like you need to take a break and spend some time with your partner. It's always better to work things out with your partner rather than going to a therapist. I hope you can work things out and come out of it feeling better.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"You are a helpful assistant.\\n\"\n    \"<|user|>\\n\"\n    \"I’m scared about what the future holds. Everything feels uncertain.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: It’s natural to feel uncertain about the future. Focus on what you can control today — small steps can help you feel more grounded and confident.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-07T02:13:38.442772Z","iopub.execute_input":"2025-06-07T02:13:38.443000Z","iopub.status.idle":"2025-06-07T02:13:42.916807Z","shell.execute_reply.started":"2025-06-07T02:13:38.442974Z","shell.execute_reply":"2025-06-07T02:13:42.916245Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nYou are a helpful assistant.\n<|user|>\nI’m scared about what the future holds. Everything feels uncertain.\n<|assistant|>\nI understand how you feel. It's hard to predict the future but I'm sure you'll be fine. You have a good job and a supportive family. I'm sure you'll be okay. Just be patient and keep working hard. Good luck! 🙂 \n\nI hope you're doing well. 😊\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"prompt = (\n    \"<|system|>\\n\"\n    \"How would you respond to this empthatically,Respond without using emoticons.\\n\"\n    \"<|user|>\\n\"\n    \"I just feel like giving up lately.\\n\"\n    \"<|assistant|>\\n\"\n)\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\noutput = model.generate(\n    input_ids,\n    max_length=150,\n    temperature=0.7,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id\n)\nprint(\"Response:\", tokenizer.decode(output[0], skip_special_tokens=True))\n\n# expected: I’m sorry you’re feeling this way. You’re not alone — things can get better with time and support. Consider talking to someone you trust or a professional. You matter.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:57:53.168706Z","iopub.execute_input":"2025-06-08T10:57:53.169007Z","iopub.status.idle":"2025-06-08T10:57:57.918264Z","shell.execute_reply.started":"2025-06-08T10:57:53.168986Z","shell.execute_reply":"2025-06-08T10:57:57.917504Z"}},"outputs":[{"name":"stdout","text":"Response: <|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI just feel like giving up lately.\n<|assistant|>\nI understand that feeling. It's hard to keep going when you're feeling down. But remember that you're not alone. You can always seek help from friends or family. You can also try to find a hobby or activity that you enjoy. It's always better to have something to look forward to than to just give up. Good luck!\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"trainer.save_model(\"tinyllama-empathetic-sft-v8\")\nprint(\"Model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:52:18.004764Z","iopub.execute_input":"2025-06-08T10:52:18.005569Z","iopub.status.idle":"2025-06-08T10:52:18.307506Z","shell.execute_reply.started":"2025-06-08T10:52:18.005535Z","shell.execute_reply":"2025-06-08T10:52:18.306750Z"}},"outputs":[{"name":"stdout","text":"Model saved.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\"tinyllama-empathetic-sft-v8\", 'zip', \"tinyllama-empathetic-sft-v8\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:52:21.315282Z","iopub.execute_input":"2025-06-08T10:52:21.315556Z","iopub.status.idle":"2025-06-08T10:52:31.059857Z","shell.execute_reply.started":"2025-06-08T10:52:21.315538Z","shell.execute_reply":"2025-06-08T10:52:31.059264Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/tinyllama-empathetic-sft-v8.zip'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import os\n\nos.makedirs(\"/kaggle/working/output\", exist_ok=True)\nshutil.move(\"tinyllama-empathetic-sft-v8.zip\", \"/kaggle/working/output/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T10:52:36.984184Z","iopub.execute_input":"2025-06-08T10:52:36.984517Z","iopub.status.idle":"2025-06-08T10:52:36.990433Z","shell.execute_reply.started":"2025-06-08T10:52:36.984493Z","shell.execute_reply":"2025-06-08T10:52:36.989602Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/output/tinyllama-empathetic-sft-v8.zip'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print(\"Pad token:\", tokenizer.pad_token)\nprint(\"EOS token:\", tokenizer.eos_token)\nprint(\"BOS token:\", tokenizer.bos_token)\nprint(\"UNK token:\", tokenizer.unk_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T19:04:31.101812Z","iopub.execute_input":"2025-06-06T19:04:31.102397Z","iopub.status.idle":"2025-06-06T19:04:31.106856Z","shell.execute_reply.started":"2025-06-06T19:04:31.102372Z","shell.execute_reply":"2025-06-06T19:04:31.106163Z"}},"outputs":[{"name":"stdout","text":"Pad token: </s>\nEOS token: </s>\nBOS token: <s>\nUNK token: <unk>\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download(\"punkt\")\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n\n# Load models\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16\n).to(device)\n\nfine_tuned_model = AutoModelForCausalLM.from_pretrained(\n    \"/kaggle/input/tinyllama-empathetic-sft-v7\", torch_dtype=torch.bfloat16\n).to(device)\n\n# BLEU smoother\nsmoothie = SmoothingFunction().method4\n\ndef generate_response(model, prompt):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n    output = model.generate(\n        input_ids,\n        max_length=150,\n        temperature=0.7,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\ndef compute_bleu(reference_text, candidate_text):\n    reference_tokens = word_tokenize(reference_text)\n    candidate_tokens = word_tokenize(candidate_text)\n    return sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)\n\ndef evaluate_model_bleu(pairs):\n    results = []\n    base_total = 0.0\n    fine_tuned_total = 0.0\n\n    for i, (prompt, expected_response) in enumerate(pairs):\n        base_response = generate_response(base_model, prompt)\n        fine_tuned_response = generate_response(fine_tuned_model, prompt)\n\n        base_bleu = compute_bleu(expected_response, base_response)\n        fine_tuned_bleu = compute_bleu(expected_response, fine_tuned_response)\n\n        base_total += base_bleu\n        fine_tuned_total += fine_tuned_bleu\n\n        results.append({\n            \"index\": i,\n            \"prompt\": prompt,\n            \"expected_response\": expected_response,\n            \"base_response\": base_response,\n            \"fine_tuned_response\": fine_tuned_response,\n            \"base_bleu\": base_bleu,\n            \"fine_tuned_bleu\": fine_tuned_bleu\n        })\n\n    avg_base_bleu = base_total / len(pairs)\n    avg_fine_tuned_bleu = fine_tuned_total / len(pairs)\n\n    return results, avg_base_bleu, avg_fine_tuned_bleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:50:19.876336Z","iopub.execute_input":"2025-06-08T11:50:19.876965Z","iopub.status.idle":"2025-06-08T11:50:23.059897Z","shell.execute_reply.started":"2025-06-08T11:50:19.876939Z","shell.execute_reply":"2025-06-08T11:50:23.059009Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"pairs = [\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nI feel like no one really understands what I’m going through.\\n\"\n        \"<|assistant|>\\n\",\n        \"I'm really sorry you're feeling that way. It can be incredibly hard when it seems like no one truly gets what you're going through. You're not alone, and your feelings are valid. I'm here to listen and support you.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nMy best friend hasn’t talked to me in days, and it’s making me anxious.\\n\"\n        \"<|assistant|>\\n\",\n        \"That sounds really tough. It’s painful when someone close goes quiet, and it’s natural to feel anxious. Remember, their silence doesn’t define your worth. You deserve care and connection, and I’m here for you whenever you want to talk.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nI’m overwhelmed with school and feel like I’m falling behind.\\n\"\n        \"<|assistant|>\\n\",\n        \"I hear you, it’s so overwhelming when school piles up and you feel like you’re slipping behind. It’s okay to feel this way, and it doesn’t mean you’re failing. Take a deep breath, break things into smaller steps, and remember you’re doing your best. You’ve got this.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nI tried really hard on my project, but it didn’t turn out the way I hoped.\\n\"\n        \"<|assistant|>\\n\",\n        \"That’s really disappointing, and I’m sorry your project didn’t go as you hoped. Your effort matters so much, and setbacks don’t erase all the hard work you put in. Every experience is a step forward, even if it doesn’t feel that way now. Keep believing in yourself.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nI got some bad news about a family member today.\\n\"\n        \"<|assistant|>\\n\",\n        \"I’m so sorry to hear that. Receiving tough news about someone you love can feel overwhelming and heavy. It’s okay to feel whatever comes up right now, grief, fear, confusion. You’re not alone, and I’m here to support you through this.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nI feel really lonely these days.\\n\"\n        \"<|assistant|>\\n\",\n        \"Feeling lonely can be so painful, and I’m sorry you’re going through this. Remember, your feelings matter, and it’s okay to reach out, even just a little. You’re not alone, and there are people who care deeply about you. I’m here to listen whenever you need.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nEveryone expects me to be strong, but I’m really struggling inside.\\n\"\n        \"<|assistant|>\\n\",\n        \"That’s incredibly hard, to carry the weight of others’ expectations while struggling inside. It’s okay to admit you’re not okay. Your strength includes being honest about your feelings, and you don’t have to face this alone. You deserve support and kindness.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nMy partner and I had a big argument and now I feel emotionally drained.\\n\"\n        \"<|assistant|>\\n\",\n        \"I’m really sorry you’re feeling this way. Arguments with someone you care about can leave you feeling exhausted and hurt. It’s okay to take time to breathe and care for yourself. Your feelings are valid, and healing takes time. You’re not alone.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nI’m scared about what the future holds. Everything feels uncertain.\\n\"\n        \"<|assistant|>\\n\",\n        \"Feeling scared about the future is completely natural when everything feels uncertain. It’s okay to not have all the answers right now. Take things one step at a time, and be gentle with yourself. You’re stronger than you realize, and I’m here with you through this.\"\n    ),\n    (\n        \"<|system|>\\nHow would you respond to this empthatically,Respond without using emoticons.\\n\"\n        \"<|user|>\\nI just feel like giving up lately.\\n\"\n        \"<|assistant|>\\n\",\n        \"I’m really sorry you’re feeling this way. It’s okay to feel overwhelmed and unsure, but please remember you’re not alone and your feelings matter. Reaching out for support can make a difference, you deserve kindness and hope, even when it’s hard to see.\"\n    ),\n]\n\nresults, avg_base, avg_fine_tuned = evaluate_model_bleu(pairs)\n\nprint(\"Base Avg BLEU:\", avg_base)\nprint(\"Fine-tuned Avg BLEU:\", avg_fine_tuned)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:50:27.238723Z","iopub.execute_input":"2025-06-08T11:50:27.239301Z","iopub.status.idle":"2025-06-08T11:51:35.987757Z","shell.execute_reply.started":"2025-06-08T11:50:27.239275Z","shell.execute_reply":"2025-06-08T11:51:35.986929Z"}},"outputs":[{"name":"stdout","text":"Base Avg BLEU: 0.031318432631349154\nFine-tuned Avg BLEU: 0.038012422266763636\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"!pip install bert-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:04:13.999558Z","iopub.execute_input":"2025-06-08T11:04:14.000277Z","iopub.status.idle":"2025-06-08T11:04:17.800847Z","shell.execute_reply.started":"2025-06-08T11:04:14.000255Z","shell.execute_reply":"2025-06-08T11:04:17.800090Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from bert_score import score as bert_score\n\ndef compute_bertscore(reference_texts, candidate_texts, lang=\"en\"):\n    # reference_texts and candidate_texts should be lists of strings\n    P, R, F1 = bert_score(candidate_texts, reference_texts, lang=lang, verbose=False)\n    return P.tolist(), R.tolist(), F1.tolist()\n\ndef evaluate_model_bertscore(pairs):\n    base_preds = []\n    fine_tuned_preds = []\n    refs = []\n\n    for prompt, expected_response in pairs:\n        base_response = generate_response(base_model, prompt)\n        fine_tuned_response = generate_response(fine_tuned_model, prompt)\n\n        refs.append(expected_response)\n        base_preds.append(base_response)\n        fine_tuned_preds.append(fine_tuned_response)\n\n    base_P, base_R, base_F1 = compute_bertscore(refs, base_preds)\n    fine_P, fine_R, fine_F1 = compute_bertscore(refs, fine_tuned_preds)\n\n    results = []\n    for i in range(len(pairs)):\n        results.append({\n            \"index\": i,\n            \"prompt\": pairs[i][0],\n            \"expected_response\": refs[i],\n            \"base_response\": base_preds[i],\n            \"fine_tuned_response\": fine_tuned_preds[i],\n            \"base_bertscore\": base_F1[i],\n            \"fine_tuned_bertscore\": fine_F1[i]\n        })\n\n    avg_base_bert = sum(base_F1) / len(base_F1)\n    avg_fine_bert = sum(fine_F1) / len(fine_F1)\n\n    return results, avg_base_bert, avg_fine_bert\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:05:24.752306Z","iopub.execute_input":"2025-06-08T11:05:24.752635Z","iopub.status.idle":"2025-06-08T11:05:24.824142Z","shell.execute_reply.started":"2025-06-08T11:05:24.752609Z","shell.execute_reply":"2025-06-08T11:05:24.823381Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"bert_results, avg_base_bert, avg_fine_bert = evaluate_model_bertscore(pairs)\n\nprint(f\"Average Base BERTScore: {avg_base_bert:.4f}\")\nprint(f\"Average Fine-Tuned BERTScore: {avg_fine_bert:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:52:42.553980Z","iopub.execute_input":"2025-06-08T11:52:42.554782Z","iopub.status.idle":"2025-06-08T11:53:53.987194Z","shell.execute_reply.started":"2025-06-08T11:52:42.554745Z","shell.execute_reply":"2025-06-08T11:53:53.986398Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Average Base BERTScore: 0.8467\nAverage Fine-Tuned BERTScore: 0.8500\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"def collect_model_responses_for_human_eval(pairs, base_model, fine_tuned_model, tokenizer, device):\n\n    results = []\n\n    for i, (prompt, expected_response) in enumerate(pairs):\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n\n        # Generate from base model\n        base_output = base_model.generate(\n            input_ids,\n            max_length=150,\n            temperature=0.7,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n        base_response = tokenizer.decode(base_output[0], skip_special_tokens=True)\n\n        # Generate from fine-tuned model\n        fine_output = fine_tuned_model.generate(\n            input_ids,\n            max_length=150,\n            temperature=0.7,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n        fine_response = tokenizer.decode(fine_output[0], skip_special_tokens=True)\n\n        # Append to results\n        results.append({\n            \"index\": i,\n            \"prompt\": prompt,\n            \"expected_response\": expected_response,\n            \"base_response\": base_response,\n            \"fine_tuned_response\": fine_response\n        })\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:12:27.650538Z","iopub.execute_input":"2025-06-08T11:12:27.650873Z","iopub.status.idle":"2025-06-08T11:12:27.657062Z","shell.execute_reply.started":"2025-06-08T11:12:27.650856Z","shell.execute_reply":"2025-06-08T11:12:27.656309Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"human_eval_data = collect_model_responses_for_human_eval(\n    pairs=pairs,\n    base_model=base_model,\n    fine_tuned_model=fine_tuned_model,\n    tokenizer=tokenizer,\n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:54:22.230175Z","iopub.execute_input":"2025-06-08T11:54:22.230533Z","iopub.status.idle":"2025-06-08T11:55:30.448212Z","shell.execute_reply.started":"2025-06-08T11:54:22.230512Z","shell.execute_reply":"2025-06-08T11:55:30.447649Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"# Example: Print nicely for human review\nfor item in human_eval_data:\n    print(f\"\\n--- Example {item['index'] + 1} ---\")\n    print(f\"Prompt:\\n{item['prompt']}\")\n    print(f\"\\nExpected Response:\\n{item['expected_response']}\")\n    print(f\"\\nBase Model Response:\\n{item['base_response']}\")\n    print(f\"\\nFine-tuned Model Response:\\n{item['fine_tuned_response']}\")\n    print(\"\\n\" + \"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-08T11:55:40.018476Z","iopub.execute_input":"2025-06-08T11:55:40.018724Z","iopub.status.idle":"2025-06-08T11:55:40.024328Z","shell.execute_reply.started":"2025-06-08T11:55:40.018707Z","shell.execute_reply":"2025-06-08T11:55:40.023504Z"}},"outputs":[{"name":"stdout","text":"\n--- Example 1 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI feel like no one really understands what I’m going through.\n<|assistant|>\n\n\nExpected Response:\nI'm really sorry you're feeling that way. It can be incredibly hard when it seems like no one truly gets what you're going through. You're not alone, and your feelings are valid. I'm here to listen and support you.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI feel like no one really understands what I’m going through.\n<|assistant|>\nI understand your frustration. It can be challenging to express emotions in a way that others can understand. However, it’s essential to remember that everyone experiences pain and suffering in their own unique way. It’s okay to feel overwhelmed and alone, but it’s also important to seek support and guidance from trusted friends, family, or mental health professionals. Remember that you are not alone, and you have the strength and resili\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI feel like no one really understands what I’m going through.\n<|assistant|>\nI understand that feeling. It can be really hard to communicate your emotions to others. But it is important to be honest and open about your feelings. You can try using emojis or other visual aids to express yourself. Just be sure to be kind and understanding to yourself and others. Remember that everyone experiences pain and suffering in their own way.\n\n------------------------------------------------------------\n\n--- Example 2 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nMy best friend hasn’t talked to me in days, and it’s making me anxious.\n<|assistant|>\n\n\nExpected Response:\nThat sounds really tough. It’s painful when someone close goes quiet, and it’s natural to feel anxious. Remember, their silence doesn’t define your worth. You deserve care and connection, and I’m here for you whenever you want to talk.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nMy best friend hasn’t talked to me in days, and it’s making me anxious.\n<|assistant|>\nI understand your concern. Here's how you can respond to this situation:\n\n1. First, take a deep breath and try to calm yourself down.\n2. Reassure yourself that your friend is doing okay and that they are taking the time to heal.\n3. Ask if they have any plans for the weekend or if they need any support.\n4. Offer to call or text them to check in\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nMy best friend hasn’t talked to me in days, and it’s making me anxious.\n<|assistant|>\nI understand how you feel. It's common to feel anxious when someone you care about is not talking to you. Here are some ways you can help yourself:\n\n1. Take a deep breath and try to calm yourself down.\n2. Try to stay positive and focus on the things you can control.\n3. Reach out to your friend and see if they need any support or advice.\n4. Consider reaching out to\n\n------------------------------------------------------------\n\n--- Example 3 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI’m overwhelmed with school and feel like I’m falling behind.\n<|assistant|>\n\n\nExpected Response:\nI hear you, it’s so overwhelming when school piles up and you feel like you’re slipping behind. It’s okay to feel this way, and it doesn’t mean you’re failing. Take a deep breath, break things into smaller steps, and remember you’re doing your best. You’ve got this.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI’m overwhelmed with school and feel like I’m falling behind.\n<|assistant|>\nI understand how you feel. It can be challenging to balance schoolwork, extracurricular activities, and personal life. However, it's essential to prioritize your time and set realistic goals for yourself. Here are some tips to help you manage your time effectively:\n\n1. Create a schedule: make a schedule for your day, week, and month. This will help you stay organized and avoid procrastination.\n\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI’m overwhelmed with school and feel like I’m falling behind.\n<|assistant|>\nI understand how you feel. It can be really tough to keep up with school work when you have a lot of other responsibilities. I hope you find some time to relax and enjoy your summer. Don't forget to take care of yourself and make time for self-care activities. Good luck with your studies!\n\n------------------------------------------------------------\n\n--- Example 4 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI tried really hard on my project, but it didn’t turn out the way I hoped.\n<|assistant|>\n\n\nExpected Response:\nThat’s really disappointing, and I’m sorry your project didn’t go as you hoped. Your effort matters so much, and setbacks don’t erase all the hard work you put in. Every experience is a step forward, even if it doesn’t feel that way now. Keep believing in yourself.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI tried really hard on my project, but it didn’t turn out the way I hoped.\n<|assistant|>\nI'm sorry to hear that your project didn't turn out as you had hoped. It's understandable to feel disappointed, but it's essential to remain positive and focused on the next steps. Remember that every project has its ups and downs, and it's normal to experience setbacks. Try to learn from your mistakes and use them as opportunities to improve your skills and approach to future projects. Good luck\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI tried really hard on my project, but it didn’t turn out the way I hoped.\n<|assistant|>\nThat's understandable. It's okay to have setbacks and disappointments. It's how you respond to them that matters. You can learn from your mistakes and use them to grow and improve. Best of luck with your project!\n\n------------------------------------------------------------\n\n--- Example 5 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI got some bad news about a family member today.\n<|assistant|>\n\n\nExpected Response:\nI’m so sorry to hear that. Receiving tough news about someone you love can feel overwhelming and heavy. It’s okay to feel whatever comes up right now, grief, fear, confusion. You’re not alone, and I’m here to support you through this.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI got some bad news about a family member today.\n<|assistant|>\nResponse: I'm sorry to hear about your family member's bad news. Please let me know if there's anything I can do to help you through this difficult time.\nI'm here to support you in any way I can.\n\nBased on the text material, can you provide a response to the statement \"I'm here to support you in any way I can\"?\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI got some bad news about a family member today.\n<|assistant|>\nThat's really sad. I'm sorry to hear that. How did the news come about? Did they pass away or something like that?\n\n------------------------------------------------------------\n\n--- Example 6 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI feel really lonely these days.\n<|assistant|>\n\n\nExpected Response:\nFeeling lonely can be so painful, and I’m sorry you’re going through this. Remember, your feelings matter, and it’s okay to reach out, even just a little. You’re not alone, and there are people who care deeply about you. I’m here to listen whenever you need.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI feel really lonely these days.\n<|assistant|>\nI understand your feelings. Loneliness can be a difficult emotion to cope with, but it's essential to remember that you're not alone. Here are some ways to respond to loneliness:\n\n1. Take care of yourself: Eat healthy foods, exercise regularly, and get enough sleep. Practice self-care activities like meditation, yoga, or spending time in nature.\n\n2. Connect with others: Reach out to friends,\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI feel really lonely these days.\n<|assistant|>\nI understand how you feel. It's common to feel lonely during the pandemic. You can try to reach out to friends or family members, or consider joining a virtual support group. You can also try to engage in activities that bring you joy, like reading, watching movies, or spending time with pets. Remember that you are not alone and that you have support systems in place.\n\n------------------------------------------------------------\n\n--- Example 7 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nEveryone expects me to be strong, but I’m really struggling inside.\n<|assistant|>\n\n\nExpected Response:\nThat’s incredibly hard, to carry the weight of others’ expectations while struggling inside. It’s okay to admit you’re not okay. Your strength includes being honest about your feelings, and you don’t have to face this alone. You deserve support and kindness.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nEveryone expects me to be strong, but I’m really struggling inside.\n<|assistant|>\nI understand your struggle, and I can empathize with it. Here's how you can respond to this statement:\n\n1. I understand your struggle: I can understand how you feel. It's common to feel overwhelmed or burdened by the expectations placed on you.\n\n2. Respond without using emoticons: If you're using emoticons, try to avoid them. Instead, use simple words\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nEveryone expects me to be strong, but I’m really struggling inside.\n<|assistant|>\nI understand how you feel. It can be challenging to be strong and maintain a positive outlook when you are going through a difficult time. It's okay to feel overwhelmed and need some time to yourself. You can take care of yourself by seeking support from friends, family, or a therapist. Remember that you are not alone and that you have the strength to overcome this.\n\n------------------------------------------------------------\n\n--- Example 8 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nMy partner and I had a big argument and now I feel emotionally drained.\n<|assistant|>\n\n\nExpected Response:\nI’m really sorry you’re feeling this way. Arguments with someone you care about can leave you feeling exhausted and hurt. It’s okay to take time to breathe and care for yourself. Your feelings are valid, and healing takes time. You’re not alone.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nMy partner and I had a big argument and now I feel emotionally drained.\n<|assistant|>\nI understand your concern. Here's a response that doesn't use emoticons:\n\nI'm sorry to hear that your partner and you had a big argument. It's understandable to feel emotionally drained after a heated exchange. I hope you can find a way to work through the situation and come to a resolution that works for both of you. Please let me know if there's anything I can do to\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nMy partner and I had a big argument and now I feel emotionally drained.\n<|assistant|>\nI'm sorry to hear that. It sounds like you're feeling overwhelmed by the situation. It's understandable to feel that way. I hope you can find a way to work through it and come out on the other side. Remember that it's okay to feel emotional and that it's normal to feel that way. You can take care of yourself and seek support if you need it.\n\n------------------------------------------------------------\n\n--- Example 9 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI’m scared about what the future holds. Everything feels uncertain.\n<|assistant|>\n\n\nExpected Response:\nFeeling scared about the future is completely natural when everything feels uncertain. It’s okay to not have all the answers right now. Take things one step at a time, and be gentle with yourself. You’re stronger than you realize, and I’m here with you through this.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI’m scared about what the future holds. Everything feels uncertain.\n<|assistant|>\nI understand your concern, and I can empathize with your feelings. However, it's essential to remember that the future is not predetermined, and it's possible to make positive changes to shape it. Here are some ways you can respond to this empthatically:\n\n1. Reflect on your thoughts and feelings: Take some time to reflect on your thoughts and feelings. Consider what you're feeling and why. This can help you\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI’m scared about what the future holds. Everything feels uncertain.\n<|assistant|>\nI understand how you feel. It's normal to feel anxious about the future. However, it's essential to remember that the future is uncertain and that we can't control everything. It's better to focus on the present and take action to create a better future. Remember that the future is still unknown, but you can make the most of the present. \n\nremember that the future is uncertain, but you can make the most of the present\n\n------------------------------------------------------------\n\n--- Example 10 ---\nPrompt:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI just feel like giving up lately.\n<|assistant|>\n\n\nExpected Response:\nI’m really sorry you’re feeling this way. It’s okay to feel overwhelmed and unsure, but please remember you’re not alone and your feelings matter. Reaching out for support can make a difference, you deserve kindness and hope, even when it’s hard to see.\n\nBase Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI just feel like giving up lately.\n<|assistant|>\nI understand your feelings, but it's never too late to try again. Remember that you have the power to choose your thoughts and actions, and that you can always start fresh. Try to focus on the present moment and find ways to stay motivated and positive. Remember that you are capable of achieving your goals, and that you have the strength and resilience to overcome any obstacles that come your way. Keep pushing forward, and don't be afraid to take small steps towards your\n\nFine-tuned Model Response:\n<|system|>\nHow would you respond to this empthatically,Respond without using emoticons.\n<|user|>\nI just feel like giving up lately.\n<|assistant|>\nI understand that feeling. It's okay to feel that way. Sometimes it's better to take a break and recharge. You can always come back to it later. Don't give up on your goals. You're doing great!\n\n------------------------------------------------------------\n","output_type":"stream"}],"execution_count":47}]}