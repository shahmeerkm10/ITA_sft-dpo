# ITA_sft-dpo
A project on SFT and DPO implementation of Tinly LLama 1.0

-The responses i judged can be found in the appropriate word documents.

-The main report is Shahmeer_Khan-25156.docx/Shahmeer_Khan-25156.pdf which contains detail about the whole experiment.

-For human evaluation i used dop-eval-1 and sft-eval, these files use CPU as i ran out of gpu access on kaggle, you can use gpu to speed things up.

-The notebooks used for experimentation are lora-ita(3) for sft and dpo-ita(2) accordingly

-Due to github upload limits the models were zipped and uploaded to the following Mega URLs:

DPO models: https://mega.nz/file/DxI1lDJI#gQok1RrzHaAN_vamD84SllgJumroj7Vilg_0UgYXiEM

SFT models: https://mega.nz/file/61Az3KAY#MHT-1HPceZU_zY4MgWaOanCkiWR93T5QeVVHQbzFZco

-Dataset citations can be found at the end of the report
