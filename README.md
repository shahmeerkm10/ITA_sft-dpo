# ITA_sft-dpo
A project on SFT and DPO implementation of Tinly LLama 1.0

-The responses i judged can be found in the appropriate word documents.

-The main report is Shahmeer_Khan-25156.docx which contains detail about the whole experiment.

-For human evaluation i used dop-eval-1 and sft-eval

-The notebooks used for experimentation are lora-ita(3) for sft and dpo-ita(2) accordingly

-Due to github upload limits the models were zipped and uploaded to the following Mega URLs:

DPO models: https://mega.nz/file/DxI1lDJI#gQok1RrzHaAN_vamD84SllgJumroj7Vilg_0UgYXiEM
SFT models: 
